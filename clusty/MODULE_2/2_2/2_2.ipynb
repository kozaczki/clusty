{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2_2\n",
    "\n",
    "\n",
    "**Function:** Using binary matrix and prime products from module 2_1 returns list of clusters per bead across all population.\n",
    "\n",
    "**Flow:** For each bead extract all pairs that are above treshold, for each pair check all potential triplets, and analogicaly quadruplets.\n",
    "\n",
    "**Input:** Binary matrix, products of primes, primes list, chromosomal borders.\n",
    "\n",
    "**Output:** List of clusters (of 4) across population per beads\n",
    "\n",
    "\n",
    "\n",
    "**Usage:** Provide path to csv file with variables and run the notebook\n",
    "\n",
    "<img src=\"2_2_R_jup.png\" alt=\"drawing\"  width=\"750\" align=\"left\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## path to parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER PATH TO CSV FILE WITH PARAMETERS ###\n",
    "path_to_parameters = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.spatial\n",
    "import ray\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import csv\n",
    "import itertools\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper funcitons\n",
    "\n",
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beads_of_interest(bead_number,neigbours_array,borders_array,structures_fraction,number_of_beads_per_structure,primes_array):\n",
    "\n",
    "    \"\"\"\n",
    "    takes a bead and returns indicies of beads that are in the neighbourhood of this bead in the desired fraction of structures\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    bead_index = bead_number - 1\n",
    " \n",
    "\n",
    "    # read chromosomal borders - first and last bead for given chromosome\n",
    "    chrom_limits = borders_array[bead_index]\n",
    "    # bead = 0 -> [1,249]\n",
    "    # bead NUMBERS\n",
    "\n",
    "    # change from ids to indexes -> therefore -1\n",
    "    first_bead_of_chromosome_index = chrom_limits[0] - 1 # beacuse it  reads x-1\n",
    "    last_bead_of_chromosome = chrom_limits[1] - 1\n",
    "\n",
    "    # pick row\n",
    "    bead_of_interest = neigbours_array[bead_index]\n",
    "\n",
    "    # get indicies of contacts above treshold\n",
    "    bead_of_interest_indicies = np.where(bead_of_interest >= structures_fraction)[0] #structes fraction because it's frequencies array\n",
    "\n",
    "    # filter for valus under these indicies\n",
    "\n",
    "\n",
    "    right_slice =  np.where(bead_of_interest_indicies < first_bead_of_chromosome_index)[0]\n",
    "    left_slice = np.where(bead_of_interest_indicies >last_bead_of_chromosome )[0]\n",
    "    left_and_right = np.append(right_slice,left_slice)\n",
    "\n",
    "    indicies = bead_of_interest_indicies[left_and_right]\n",
    "\n",
    "    # RETURNS INDICES OF BEADS\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    return indicies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def pick_fours(bead_number,neighbours_array,borders,product_folder,structures_fraction,number_of_beads_per_structure,primes_array):\n",
    "\n",
    "    # bead numbers         [1,2,3,4,5]\n",
    "    # primes_array indexes [0,1,2,3,4]\n",
    "    # primes               [2,3,5,7,11]\n",
    "\n",
    "    # load a product for given bead (product was saved under bead id)\n",
    "\n",
    "    prod = np.load(product_folder + '/prod_' + str(bead_number) +'.npy',allow_pickle=True)\n",
    "\n",
    "    # add triplets here\n",
    "\n",
    "    bead_triplets = []\n",
    "\n",
    "    bead_index = bead_number - 1\n",
    "\n",
    "\n",
    "    # get all indicies of bead in trans chromosomes\n",
    "    bead_x_indicies = get_beads_of_interest(bead_number,neighbours_array,borders,structures_fraction,number_of_beads_per_structure,primes_array) # indices\n",
    "    #get all possible doublets (in reality they are triplets, because of bead of origin)\n",
    "    bead_0_indicies_all = [i for i in range(number_of_beads_per_structure) if (neighbours_array[bead_index,i] >= structures_fraction and i != bead_index)]\n",
    "\n",
    "    # combine cis and trans\n",
    "    bead_0_indicies_all_combined = list(combinations(bead_0_indicies_all,2))\n",
    "\n",
    "    # filter combined for possible triplets\n",
    "    bead_0_indicies_all_filtered = [(i[0],i[1]) for i in bead_0_indicies_all_combined if neighbours_array[i[0],i[1]] >= structures_fraction ]\n",
    "\n",
    "    bead_0_indicies_all_filtered_subset = bead_0_indicies_all_filtered\n",
    "\n",
    "    # bead numbers         [1,2,3,4,5]\n",
    "    # primes_array indexes [0,1,2,3,4]\n",
    "    # primes               [2,3,5,7,11]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # neighbours for bead\n",
    "    bead_0_str = prod\n",
    "    primes_inter = primes_array[bead_x_indicies] # trans beads\n",
    "    for i in bead_0_indicies_all_filtered_subset: # all doublets that passed >= fraction\n",
    "        product = primes_array[i[0]]*primes_array[i[1]] # doublet as primes\n",
    "        mods = np.mod(bead_0_str,product) # array with remainders\n",
    "        ins = np.where(mods == 0)[0] # indicies of structures in which they are present\n",
    "        ins_sum = ins.shape\n",
    "   \n",
    "        if ins_sum[0] >= structures_fraction: # if for given doublet there is more than hundred structures\n",
    "\n",
    "        # subset of structures where doublet is present\n",
    "\n",
    "            bead_0_str_subset = bead_0_str[ins]    # subset of strucutres to check\n",
    "\n",
    "            potential_triplets_product = primes_inter * product  # THAT'S WHERE TRANS ARE ADDED\n",
    "\n",
    "            potential_triplets_product_full = np.full((bead_0_str_subset.shape[0],potential_triplets_product.shape[0]),potential_triplets_product)\n",
    "            mods_2 = np.mod(bead_0_str_subset,potential_triplets_product_full.T) # remainders for triplets\n",
    "\n",
    "       #mods_2_reshape = np.reshape(mods_2(products_2.shape[0],))\n",
    "\n",
    "            ins_2_0 = np.where(mods_2 == 0)[0] # by 3rd element [indexes of ]\n",
    "            ins_2_1 = np.where(mods_2 == 0)[1] # by structure number\n",
    "\n",
    "            val,counts = np.unique(ins_2_0,return_counts=True)\n",
    "\n",
    "\n",
    "            counts_more_than_hundres = np.where(counts >= structures_fraction)\n",
    "\n",
    "   \n",
    "            indexes_of_primes_sybset_passing_the_treshold = val[counts_more_than_hundres[0]]\n",
    "\n",
    "            #print(indexes_of_primes_sybset_passing_the_treshold)\n",
    "            pr = primes_inter[indexes_of_primes_sybset_passing_the_treshold]\n",
    "        #print(pr)\n",
    "        #print(i)\n",
    "            for p in pr:\n",
    "                t = (int(np.where(primes_array==p)[0]))\n",
    "                if (i[0] != i[1] and i[0] != t and i[1] != t): # retruns index of prime, to get bead: add 1\n",
    "                    #bead_triplet = [i[0],i[1],t]\n",
    "                    bead_triplet = [i[0]+1,i[1]+1,t+1] # returns beads IDS\n",
    "\n",
    "                    bead_triplet_sorted = tuple(sorted(bead_triplet))\n",
    "                    bead_triplets.append(bead_triplet_sorted)\n",
    "\n",
    "\n",
    "    if len(bead_triplets) > 0:\n",
    "        print(bead_number,len(bead_triplets))\n",
    "   \n",
    "\n",
    "\n",
    "    return (bead_number, bead_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_clusters(cluster):\n",
    "    filtered_clusteres = []\n",
    "    for triplet in cluster[1]:\n",
    "        triplet_list = list(triplet)\n",
    "        triplet_list_sorted = sorted(triplet_list)\n",
    "        if triplet_list_sorted not in filtered_clusteres:\n",
    "            filtered_clusteres.append(triplet_list_sorted)\n",
    "    return cluster[0],filtered_clusteres "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading parameters, building folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load paramaters from csv file \n",
    "\n",
    "paramaters = []\n",
    "with open(path_to_parameters, 'rt') as csvfile:\n",
    "    reader = csv.reader(csvfile, skipinitialspace=True)\n",
    "    paramaters.append(list(reader))\n",
    "    csvfile.close()\n",
    "\n",
    "params = paramaters[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign setup variebles from params\n",
    "\n",
    "home = params[0][1]\n",
    "number_of_structures = int(params[1][1])\n",
    "number_of_beads_per_structure = int(params[2][1])\n",
    "fraction = float(params[3][1])\n",
    "structures_fraction = number_of_structures * fraction\n",
    "cores = int(params[4][1])\n",
    "dataset_name =  params[6][1]\n",
    "dataset_folder =  params[7][1]\n",
    "a_type = params[8][1]\n",
    "\n",
    "chromosomal_borders_file = params[9][1]\n",
    "primes_file = params[10][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle dataset_name depending on analysis_type\n",
    "\n",
    "if a_type == 'fixed':\n",
    "    r_factor  = float(params[5][1])\n",
    "    analysis_name = dataset_name + '_fixed_radius_' + str(r_factor)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "if a_type == 'neighbours':\n",
    "    k  = int(params[5][1])\n",
    "    analysis_name = dataset_name + '_neighbours_' + str(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print setup variables for manual inspection\n",
    "print(\"\")\n",
    "print(\"Running population-wide cluster detection\")\n",
    "print(\"\")\n",
    "print(\"analysis name: \" + analysis_name)\n",
    "\n",
    "print(\"loaded setup variables\")\n",
    "print(\"\")\n",
    "print(\"home folder: \" + home)\n",
    "print(\"dataset folder: \" + dataset_folder)\n",
    "print(\"dataset name: \" + dataset_name)\n",
    "print(\"number of structures: \" + str(number_of_structures))\n",
    "print(\"number of beads per structure: \" + str(number_of_beads_per_structure))\n",
    "print(\"fraction: \" + str(fraction))\n",
    "\n",
    "if a_type == \"fixed\":\n",
    "    print(\"radius factor: \" + str(r_factor))\n",
    "if a_type == 'neighbours':\n",
    "    print(\"k: \" + str(k))\n",
    "\n",
    "print(\"cores: \" + str(cores))\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defin regex patter for csv files\n",
    "pattern = '^(.*)cf_(.*).coords.csv$'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_folder = os.path.join(home,'runs',analysis_name)\n",
    "intermediate_files_folder = os.path.join(run_folder,'intermediate')\n",
    "product_folder = os.path.join(intermediate_files_folder,\"products\")\n",
    "results_folder = os.path.join(run_folder,'results')\n",
    "figures_folder = os.path.join(run_folder,'figures')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stored input data\n",
    "\n",
    "#  SAVE FILES\n",
    "binary = np.load(os.path.join(intermediate_files_folder,analysis_name + '_binary.npy'))\n",
    "frequencies = np.load(os.path.join(intermediate_files_folder,analysis_name + '_frequencies.npy'))\n",
    "\n",
    "# HELPER DATA FILES\n",
    "\n",
    "helper_folder = os.path.join(home,'helper_data')\n",
    "\n",
    "\n",
    "\n",
    "# load and process helper data\n",
    "\n",
    "primes_array = np.load(os.path.join(helper_folder,primes_file),allow_pickle=True)\n",
    "\n",
    "# CHROMOSOMAL BORDERS ARRAY\n",
    "\n",
    "chromosomal_borders = np.load(os.path.join(helper_folder,chromosomal_borders_file))\n",
    "\n",
    "# get chromosomal borders\n",
    "\n",
    "borders_array = np.unique(chromosomal_borders)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## identify cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# begin multiprocessing\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "results = []\n",
    "\n",
    "path_to_store = intermediate_files_folder + analysis_name +  \"_final_clusters_running\"\n",
    "\n",
    "# process bead by bead\n",
    "\n",
    "for chunk in chunker(range(number_of_beads_per_structure),cores):    \n",
    "\n",
    "    ids = [pick_fours.remote((bi+1),frequencies,chromosomal_borders,product_folder,structures_fraction,number_of_beads_per_structure,primes_array) for bi in chunk]\n",
    "    partial_results = ray.get(ids)\n",
    "    results.append(partial_results)\n",
    "\n",
    "    file_to_store = open(path_to_store, \"wb\")\n",
    "    pickle.dump(results, file_to_store)\n",
    "\n",
    "    file_to_store.close()\n",
    "\n",
    "print(\"cluster identification took \" + str(time.time() - start) + \" s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all the lists together\n",
    "\n",
    "import itertools\n",
    "clusters_list = results[0]\n",
    "for i in range(1,len(results)):\n",
    "    clusters_list = list(itertools.chain(clusters_list,results[i]))\n",
    "    \n",
    "filtered_clusters = [filter_clusters(i) for i in clusters_list]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare figures, save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "\n",
    "filtered_clusters_list_to_save_path = os.path.join(results_folder,analysis_name + '_clusters_simple_filtered')\n",
    "file_to_store = open(filtered_clusters_list_to_save_path, \"wb\")\n",
    "pickle.dump(filtered_clusters, file_to_store)\n",
    "file_to_store.close()\n",
    "\n",
    "clusters_list_to_save = list(itertools.chain(*results[0]))\n",
    "clusters_list_to_save_path = os.path.join(results_folder,analysis_name + '_clusters_simple')\n",
    "file_to_store = open(clusters_list_to_save_path, \"wb\")\n",
    "pickle.dump(clusters_list_to_save, file_to_store)\n",
    "file_to_store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat data for plot with clusters distribution\n",
    "beads_numbers = [i[0] for i in clusters_list]\n",
    "cluster_numbers = [len(i[1]) for i in clusters_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat data for plot with FILTERED clusters distribution\n",
    "beads_numbers_filtered = [i[0] for i in filtered_clusters]\n",
    "cluster_numbers_filtered = [len(i[1]) for i in filtered_clusters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare plot data for plot with clusters distribution\n",
    "fig, ax = plt.subplots(2,1,figsize = (20,5))\n",
    "ax[0].scatter(beads_numbers,cluster_numbers)\n",
    "ax[1].scatter(beads_numbers_filtered,cluster_numbers_filtered)\n",
    "\n",
    "\n",
    "fig.savefig(os.path.join(figures_folder,analysis_name + '_clusters_distribution.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save final clusters\n",
    "\n",
    "path_to_store = intermediate_files_folder + analysis_name +  \"final_clusters\"\n",
    "\n",
    "\n",
    "final_clusters = (results,beads_numbers,cluster_numbers)\n",
    "\n",
    "file_to_store = open(path_to_store, \"wb\")\n",
    "pickle.dump(final_clusters, file_to_store)\n",
    "\n",
    "file_to_store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
