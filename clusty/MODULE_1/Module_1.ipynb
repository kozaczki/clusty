{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cb33f75",
   "metadata": {},
   "source": [
    "# Module 1- **in silico Hi-C**\n",
    "\n",
    "**Function:** generation of in silico Hi-C matrix using the population of structures. \n",
    "\n",
    "**Data Flow:** Each structure is converted to a graph representation. Then using Leiden's algorithm it divides each graph into communities. Finally for each pair of beads if checks in how many structures they are members of the same community and generates in silico HiC matrix.\n",
    "\n",
    "**Input:** population of structures (filename format: cf_XXXXXX.coords.csv, XXXXXX - structure id completed to 6 characters with zeros)\n",
    "\n",
    "**Output:** in silico HiC matrix (npz format), list of communities and their members\n",
    "\n",
    "**Usage:** Provide path to csv file with variables and run the notebook\n",
    "\n",
    "<div>\n",
    "<img src=\"module_1_dataflow_jup.png\" alt=\"drawing\"  align=\"center\" width =\"250\"/>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91c5a9c",
   "metadata": {},
   "source": [
    "# path to csv with paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ebf9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER PATH TO CSV FILE WITH PARAMETERS ###\n",
    "path_to_parameters = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b5e12e",
   "metadata": {},
   "source": [
    "# libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e7e11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import numpy as np\n",
    "import umap\n",
    "import umap.umap_ as umap\n",
    "\n",
    "import cdlib\n",
    "from cdlib import algorithms\n",
    "import networkx as nx\n",
    "from  scipy import sparse\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import seaborn as sns;\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import ray\n",
    "from scipy.sparse import csr_matrix\n",
    "import csv\n",
    "from networkx import Graph\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d992c27",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70d68af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divides a sequence type into chunks\n",
    "\n",
    "def chunker(seq, size):\n",
    "    \"\"\"\n",
    "    input: sequence (eg. list)\n",
    "    ouptput: sequence divided in chunks\n",
    "    parameters: size: determines the length of the chunk\n",
    "    \"\"\"\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b481b738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper decorator function for benchmarking\n",
    "\n",
    "def my_timer(func):\n",
    "    \"\"\"\n",
    "    input: function to be benchmarked\n",
    "    ouptput: execution time of benchmarked function\n",
    "    \"\"\"\n",
    "    def wrapper(*args,**kwargs):\n",
    "        t_start = time.time()\n",
    "        result = func(*args,**kwargs)\n",
    "        t_end = time.time() - t_start\n",
    "        print('{} took {}s'.format(func.__name__, t_end))\n",
    "\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe36bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(XYZ,k):\n",
    "    \"\"\"\n",
    "    Input: an array of (x,y,z) coordinates\n",
    "    Output: the weighted adjacency matrix of the UMAP graph representation\n",
    "    Parameters: k is the most important parameter in the umap fuzzy_simplicial_set function. \n",
    "    It will determine how sparse the final graph will be.\n",
    "    \"\"\"\n",
    "#    umap.umap_.fuzzy_simplicial_set\n",
    "    adj = umap.fuzzy_simplicial_set(\n",
    "        XYZ,\n",
    "        n_neighbors=k, # this parameter has to be fine-tuned\n",
    "        random_state=np.random.RandomState(seed=42),\n",
    "        metric='l2',\n",
    "        metric_kwds={},\n",
    "        knn_indices=None,\n",
    "        knn_dists=None,\n",
    "        angular=False,\n",
    "        set_op_mix_ratio=1.0,\n",
    "        local_connectivity=2.0,\n",
    "        verbose=False,\n",
    "        \n",
    "        )\n",
    "  \n",
    "    return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b94b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_communities(adj):\n",
    "    \"\"\"\n",
    "    Input: the weighted graph adjacency matrix\n",
    "    Output: a list of communities, each one a represented as a list object\n",
    "    leiden algorithm as implemented in the cdlib library.\n",
    "    \"\"\"\n",
    "    # generate a graph networkx obj\n",
    "    g = nx.from_scipy_sparse_matrix(adj) \n",
    "    # get list of edges from graph\n",
    "    eset = [(u, v) for (u, v, d) in g.edges(data=True)]\n",
    "    # get list of weights from edges\n",
    "    weights = [d['weight'] for (u, v, d) in g.edges(data=True)] \n",
    "    # find communities using Leiden alg\n",
    "    leiden_coms = algorithms.leiden(g,weights=weights) \n",
    "    # a list of lists of nodes\n",
    "    return leiden_coms.communities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b83b822",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parrarel graph aggregator\n",
    "\n",
    "@ray.remote\n",
    "def aggregate_graphs(graph1,graph2):\n",
    "    return graph1 + graph2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad97ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure -> graph -> communities\n",
    "\n",
    "@ray.remote\n",
    "\n",
    "def read_and_prepare_graph_and_communities(folder,num,k,number_of_beads_per_structure):\n",
    "    \n",
    "    '''\n",
    "    input: csv file with coordinates for single structure\n",
    "    ouput: graph, list of communities and id of structure (from csv name)\n",
    "    parameters: k - is the most important parameter in the umap fuzzy_simplicial_set function. \n",
    "    It will determine how sparse the final graph will be.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    #obtain id of processed_structur\n",
    "  \n",
    "    \n",
    "    filename = 'cf_' + str(num).zfill(6) + '.coords.csv'\n",
    "    file = os.path.join(folder,filename)\n",
    "    \n",
    "    \n",
    "    # read csv into np.array\n",
    "    coordinates = np.genfromtxt(file, delimiter= ',')\n",
    "    # get columns for x,y,z coordinates\n",
    "    coordinates_xyz = coordinates[:,:3]\n",
    "    # build a graph from x,y,z\n",
    "    graph = build_graph(coordinates_xyz,k)\n",
    "    # detect communities\n",
    "    communities =  build_communities(graph[0])\n",
    "    # communities are list of lists of lists : community / beads\n",
    "    # obtained communities are used for as input to build a complete graph for given structure\n",
    "    for community_index in range(len(communities)):\n",
    "        # for the first community build graph\n",
    "        if community_index == 0:\n",
    "            community_graph = nx.complete_graph(communities[community_index]) \n",
    "        else:\n",
    "         # for the following communities update graph\n",
    "            community_graph.update(nx.complete_graph(communities[community_index]))\n",
    "         #once done - return graph , list of communities and id of processed structure\n",
    "    return nx.to_scipy_sparse_matrix(community_graph,nodelist=range(number_of_beads_per_structure)) , communities , num\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6098bc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "@my_timer \n",
    "\n",
    "# parallelizing the whole process\n",
    "\n",
    "def process_csvs(folder,cores,k,number_of_beads_per_structure):\n",
    "    \n",
    "    '''\n",
    "    input: folder with csvs\n",
    "    ouput: in silico HiC matrix for the population of structures in input folder, dictionary with communities\n",
    "    \n",
    "    parameters: \n",
    "    \n",
    "    k - is the most important parameter in the umap fuzzy_simplicial_set function. \n",
    "    It will determine how sparse the final graph will be.\n",
    "    cores - number of cores available\n",
    "    number_of_beads_per_structure  \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # list to accumulate matrices\n",
    "    results = []\n",
    "    \n",
    "    #counter to follow progres \n",
    "    counter = 0\n",
    "    \n",
    "    #dictionary for str:list of communities\n",
    "    communities_ditc = {}\n",
    "    \n",
    "    #process multi\n",
    "    for chunk in chunker(range(1,number_of_structures+1),cores):\n",
    "        \n",
    "        #initiate separate processes for each file in chunk \n",
    "        ids = [read_and_prepare_graph_and_communities.remote(folder,num,k,number_of_beads_per_structure) for num in chunk]\n",
    "        # list to accumulate matrices from each file in chunk\n",
    "        partial_results = []\n",
    "        # get results from processes\n",
    "        partial_results_triple = ray.get(ids)\n",
    "        # wait till all processes are done\n",
    "        ready, not_ready = ray.wait(ids,num_returns= len(chunk))\n",
    "        # for each matrix,communities,structure_id\n",
    "        for triple in partial_results_triple:\n",
    "            # add entry structure_id : communities to communities dictionary\n",
    "            communities_ditc[triple[2]] = triple[1]\n",
    "            # add matrix to partial results\n",
    "            partial_results.append(triple[0])\n",
    "            \n",
    "        \n",
    "        \n",
    "        # aggregate matrices to one sigle matrix (to save memory)\n",
    "        while len(partial_results) > 1:\n",
    "            partial_results = partial_results[2:] + [aggregate_graphs.remote(partial_results[0], partial_results[1])]\n",
    "        folder_chunk_results = ray.get(partial_results[0])\n",
    "        \n",
    "        #append aggregated matrix to final results\n",
    "        results.append(folder_chunk_results)\n",
    "     \n",
    "        #track progress\n",
    "        counter += cores\n",
    "        if counter%1000 == 0:\n",
    "            print(counter)\n",
    "        \n",
    "        # aggregate final matrices (to save memory)\n",
    "        while len(results) > 1:\n",
    "            results = results[2:] + [aggregate_graphs.remote(results[0], results[1])]\n",
    "        \n",
    "            \n",
    "    return results,communities_ditc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8465674b",
   "metadata": {},
   "source": [
    "## loading parameters, building folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e687f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load paramaters from csv file \n",
    "\n",
    "# parse csv file with parameters\n",
    "paramaters = []\n",
    "with open(path_to_parameters, 'rt') as csvfile:\n",
    "    reader = csv.reader(csvfile, skipinitialspace=True)\n",
    "    paramaters.append(list(reader))\n",
    "    csvfile.close()\n",
    "\n",
    "#list with setup parameters\n",
    "params = paramaters[0]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976bfb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign setup variebles from params\n",
    "\n",
    "home = params[0][1]\n",
    "number_of_structures = int(params[1][1])\n",
    "number_of_beads_per_structure = int(params[2][1])\n",
    "cores = int(params[3][1])\n",
    "k = int(params[4][1])\n",
    "dataset_name =  params[5][1]\n",
    "dataset_folder =  params[6][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a211191f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compose analysis name\n",
    "\n",
    "analysis_name = dataset_name + '_inSilico_' + str(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37096148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print loaded parameters:\n",
    "\n",
    "print('Analysis name: ' + str(analysis_name))\n",
    "print('Home directory : ' + str(home))\n",
    "print('Dataset name :' + dataset_name)\n",
    "print('Dataset directory: ' + dataset_folder)\n",
    "print('number of structures: ' + str(number_of_structures))\n",
    "print('number of beads per structure: ' + str(number_of_beads_per_structure))\n",
    "print('cores: ' + str(cores))\n",
    "print('k for graph: ' + str(k))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6d4161",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build folders structure\n",
    "\n",
    "run_folder = os.path.join(home,'runs',analysis_name)\n",
    "results_folder = os.path.join(run_folder,'results')\n",
    "figures_folder = os.path.join(run_folder,'figures')\n",
    "\n",
    "print(\"building folders structure for the run\")\n",
    "\n",
    "\n",
    "folders = [run_folder,results_folder,figures_folder]\n",
    "\n",
    "for folder in folders:\n",
    "    try:\n",
    "        os.mkdir(folder)\n",
    "        print(\"Directory \" , folder ,  \" Created \")\n",
    "    except FileExistsError:\n",
    "        print(\"Directory \" , folder ,  \" already exists\")\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a00be99",
   "metadata": {},
   "source": [
    "## in silico HiC matrix preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b705e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize ray\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b65fe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run algorith on chosen dataset \n",
    "\n",
    "# process all structures\n",
    "\n",
    "inSilicoHiC,communities_dict = process_csvs(dataset_folder,cores,k,number_of_beads_per_structure)\n",
    "\n",
    "# get results  \n",
    "\n",
    "out = ray.get(inSilicoHiC[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a1023f",
   "metadata": {},
   "source": [
    "## saving files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ede6447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize matrix\n",
    "\n",
    "# transform hic matrix to dense form\n",
    "PA_dense = sparse.csr_matrix.todense(out)\n",
    "# set up visualization theme\n",
    "sns.set_theme()\n",
    "# figsize in inches\n",
    "fig, ax = plt.subplots(figsize=(20,15))         \n",
    "ax = sns.heatmap(PA_dense)\n",
    "# set up figure title\n",
    "title = \"in silico HiC for \" + dataset_name + \" k=\" + str(k)\n",
    "fig.suptitle(title)\n",
    "# save figure\n",
    "fig.savefig(os.path.join(figures_folder,'hic_' + analysis_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b929540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save communities\n",
    "\n",
    "# set up path to communities\n",
    "file_to_store_communities = open(os.path.join(results_folder,'communities_' + analysis_name),'wb')\n",
    "# save and close the file\n",
    "pickle.dump(communities_dict,file_to_store_communities)\n",
    "file_to_store_communities.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8696a708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save matrix\n",
    "\n",
    "# set up path to matrix\n",
    "file_to_store__HIC = os.path.join(results_folder,'HIC_matrix_' + analysis_name)\n",
    "\n",
    "# save file\n",
    "sparse.save_npz(file_to_store__HIC,out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
